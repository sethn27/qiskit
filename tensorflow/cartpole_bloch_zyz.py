# -*- coding: utf-8 -*-
"""CartPole_Bloch_ZYZ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_GzzCM1uoL2JDdMcLYxgqUmkEL8Ds6ks
"""

# !pip install --upgrade pip
# !pip install tensorflow==2.7.0

# !pip install tensorflow_quantum

# !pip install gym==0.18.0
# !pip install qiskit

import gym
import numpy as np
import random
import matplotlib.pyplot as plt
import math
from collections import deque
import tensorflow as tf
import tensorflow_quantum as tfq
import cirq
import sympy

import numpy as np
import matplotlib.pyplot as plt
from qiskit.circuit import QuantumCircuit, Parameter
from qiskit.circuit import ParameterVector

# to each circuit, and plot them on the Bloch sphere:
from qiskit.visualization.bloch import Bloch
from qiskit.quantum_info import Statevector



number_of_reuse = 64
class PPO_agent(object):
    def __init__(self, action_space, state_space) -> None:
        super().__init__()
        self.action_space = action_space
        self.state_space = state_space[0]
        self.e = 0.1  # Policy distance
        self.qubits = [cirq.GridQubit(0, i) for i in range(1)]
        self.gamma = 0.98  # Discount factor
        self.K = 4  # Number of epochs
        self.T = 500  # Horizon
        self.M = 64  # Batch size
        self.ent = 0.1
        self.states = np.zeros((self.T, self.state_space))
        self.rewards = np.zeros((self.T, 1))
        self.actions = np.zeros((self.T, 1))
        self.probs = np.zeros((self.T, self.action_space))
        self.iter = 0
        self.policy_opt = tf.keras.optimizers.Adam(lr=0.004)
        self.critic_opt = tf.keras.optimizers.Adam(lr=0.04)
        self.actor, self.critic = self.make_func_approx()
        
    def make_func_approx(self):
        readout_operators_actor = []
        readout_operators_value = []
        for i in range(number_of_reuse):
            readout_operators_actor.append(cirq.Z(self.qubits[0]))
        
        inputs = tf.keras.Input(shape=(), dtype=tf.dtypes.string)
        diff = tfq.differentiators.ParameterShift()
        diff1 = tfq.differentiators.ParameterShift()

        init = tf.keras.initializers.Zeros

        print('Actor')
        Actor_output = tfq.layers.PQC(self.make_circuit(self.qubits), readout_operators_actor, differentiator=diff,
                             initializer=init)(inputs)

        Actor_output = tf.keras.layers.Dense(2, activation='linear')(Actor_output)
        Actor_output_softmax = tf.keras.layers.Softmax()(Actor_output)
        Actor_model = tf.keras.Model(inputs=[inputs], outputs=[Actor_output_softmax])
        for i in range(number_of_reuse):
            readout_operators_value.append(cirq.Z(self.qubits[0]))
        
        print('Critic')
        critic_output = tfq.layers.PQC(self.make_circuit(self.qubits), readout_operators_value, differentiator=diff1,
                                initializer=tf.keras.initializers.Zeros)(inputs)

        critic_output_bias = tf.keras.layers.Dense(1, activation='linear')(critic_output)
        Critic_model = tf.keras.Model(inputs=[inputs], outputs=[critic_output_bias])
        Actor_model.summary()
        Critic_model.summary()
        
        return Actor_model,Critic_model



    def layer_no_etanglement(self, weights, qubits):
        l = cirq.Circuit()
        l.append([cirq.Moment([cirq.rx(weights[j]).on(qubits[j]) for j in range(1)])])
        
        #l.append([cirq.Moment([cirq.ry(weights[j+4]).on(qubits[j]) for j in range(4)])])

        return l

    def convert_data(self, x, flag=True):
        ops = cirq.Circuit()
        beta0 = math.atan(x[0])
        beta1 = math.atan(x[1])
        beta2 = math.atan(x[2])
        beta3 = math.atan(x[3])
        
          
        ops.append([cirq.Moment([cirq.H(self.qubits[j]) for j in range(1)])])
        for j in range(1):
            
            ops.append(cirq.rz(beta1).on(self.qubits[j]))
            ops.append(cirq.ry(beta2).on(self.qubits[j]))
            ops.append(cirq.rz(beta3).on(self.qubits[j]))
        #print(ops)
        if flag:
            return tfq.convert_to_tensor([ops])
        else:
            return ops

    def make_circuit(self, qubits):
        m = cirq.Circuit()
        symbols = sympy.symbols('q0:8') 
        m += self.layer_no_etanglement(symbols[0:], qubits)

        print(m)
        return m

    def remember(self, state, reward, action, probs):
        
        i = self.iter
        self.states[i] = state
        self.rewards[i] = reward
        self.actions[i] = action
        self.probs[i] = probs
        self.iter += 1
        

    def get_action(self, obs):
        probs = self.actor.predict(self.convert_data(obs))[0]
        value = self.critic.predict(self.convert_data(obs))[0]
        action = np.random.choice(self.action_space, p=probs)

        return action, probs

    def discount_reward(self, rewards):
        d_rewards = np.zeros_like(rewards)
        Gt = 0
        # Discount rewards
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                Gt = 0
            else:
                Gt = rewards[i] + self.gamma * Gt
            d_rewards[i] = Gt
        return d_rewards

    def entropy(self, probs):
        return tf.reduce_mean(-probs * tf.math.log(probs))

    def ppo_loss(self, cur_pol, old_pol, advantages):

        ratio = cur_pol/old_pol
        ratio = tf.clip_by_value(ratio, 1e-10, 10 - 1e-10)
        clipped = tf.clip_by_value(ratio, 1 - self.e, 1 + self.e)

        loss = -tf.reduce_mean(tf.math.minimum(ratio * advantages, clipped * advantages))

        return loss
    def train(self):
        batch_indices =[i for i in range(self.iter)]

        state_batch = tfq.convert_to_tensor([self.convert_data(i, False) for i in self.states[batch_indices]])

        p_batch = tf.convert_to_tensor(self.probs[:self.iter])
        action_batch = tf.convert_to_tensor(self.actions[:self.iter])
        action_batch = [[i, action_batch[i][0]] for i in range(len(action_batch))]
        p_batch = tf.cast(p_batch, dtype=tf.float32)
   
        action_batch = tf.cast(action_batch, dtype=tf.int32)

        rewards = self.discount_reward(self.rewards[:self.iter])

        for _ in range(self.K):
            with tf.GradientTape() as value_tape:
                value_prediction = self.critic(state_batch, training=True)
                critic_loss = tf.math.reduce_mean(tf.math.square(value_prediction - rewards))
            critic_grads = value_tape.gradient(critic_loss, self.critic.trainable_variables)
            self.critic_opt.apply_gradients(zip(critic_grads, self.critic.trainable_variables))

            #print('critic is',self.critic.trainable_variables)

            with tf.GradientTape() as policy_tape:
                advantages = rewards - value_prediction
                advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8) #advantages normalized
                policy_pred = self.actor(state_batch, training=True)
                policy_loss = self.ppo_loss(tf.gather_nd(policy_pred, action_batch),
                                            tf.gather_nd(p_batch, action_batch), tf.squeeze(advantages))

            policy_grads = policy_tape.gradient(policy_loss, self.actor.trainable_variables)
            self.policy_opt.apply_gradients(zip(policy_grads, self.actor.trainable_variables))

            #print('actor is',self.actor.trainable_variables)
            train_param = self.actor.trainable_variables

        self.iter = 0
        return train_param

import numpy as np
import matplotlib.pyplot as plt
from qiskit.circuit import QuantumCircuit, Parameter
from qiskit.circuit import ParameterVector

# to each circuit, and plot them on the Bloch sphere:
from qiskit.visualization.bloch import Bloch
from qiskit.quantum_info import Statevector
from qiskit.visualization import plot_state_qsphere, plot_bloch_multivector

if __name__ == '__main__':
    

    # First, we need to define the circuits:
    theta_param = Parameter('θ')
    phi_param = Parameter('Φ')
    beta1_param = Parameter('b1')
    beta2_param = Parameter('b2')
    beta3_param = Parameter('b3')
    beta4_param = Parameter('b4')

    #CHANGE GATES HERE
    # Circuit A
    qc_A = QuantumCircuit(1)
    qc_A.h(0)

    qc_A.rz(beta1_param, 0)
    qc_A.ry(beta2_param, 0)
    qc_A.rz(beta3_param, 0)
    # Circuit B
    qc_B = QuantumCircuit(1)
    qc_B.h(0)

    qc_B.rz(beta1_param, 0)
    qc_B.ry(beta2_param, 0)
    qc_B.rz(beta3_param, 0)

    qc_B.rx(theta_param, 0)

    # Bloch sphere plot formatting
    
    b1 = Bloch()
    b2 = Bloch()
    b1.point_color = ['tab:blue']
    b2.point_color = ['tab:blue']
    b1.point_marker = ['o']
    b2.point_marker = ['o']
    b1.point_size=[2]
    b2.point_size=[2]
    

    def state_to_bloch(state_vec):
    # Converts state vectors to points on the Bloch sphere
      phi = np.angle(state_vec.data[1])-np.angle(state_vec.data[0])
      theta = 2*np.arccos(np.abs(state_vec.data[0]))
      return [np.sin(theta)*np.cos(phi),np.sin(theta)*np.sin(phi),np.cos(theta)]

    obs1 = []
    obs2 = []
    obs3 = []

    ITERATIONS = 150
    windows = 20
    env = random.seed(34)

    env = gym.make("CartPole-v1")
    '''env.observation_space.shape'''
    agent = PPO_agent(env.action_space.n, env.observation_space.shape)
    plot_rewards = []
    avg_reward = deque(maxlen=ITERATIONS)
    best_avg_reward = -math.inf
    rs = deque(maxlen=windows)

    for i in range(ITERATIONS):
        s1 = env.reset()
        episode_reward = 0
        done = False


        # Bloch sphere plot formatting   
        b1 = Bloch()
        b2 = Bloch()
        b1.point_color = ['tab:blue']
        b2.point_color = ['tab:blue']
        b1.point_marker = ['o']
        b2.point_marker = ['o']
        b1.point_size=[2]
        b2.point_size=[2]


        while not done:
            # env.render()

            obs1.append(math.atan(s1[1]))
            obs2.append(math.atan(s1[2]))
            obs3.append(math.atan(s1[3]))
                           

            action, p = agent.get_action(s1)
            s2, reward, done, info = env.step(action)
            episode_reward += reward
            agent.remember(s1, reward, action, p)
            s1 = s2


        #agent.train()

        for e in range(len(obs1)):  
            state_1=Statevector.from_instruction(qc_A.bind_parameters({beta1_param:obs1[e], beta2_param:obs2[e], beta3_param:obs3[e]}))          
            b1.add_points(state_to_bloch(state_1))            
        b1.show()
        
        
        para_raw = agent.train()

        #Trainable Param
        para = para_raw[0].numpy()[0] 
        print("Trainable parameter:", para)

        for n in range(len(obs1)):        
          state_2=Statevector.from_instruction(qc_B.bind_parameters({beta1_param:obs1[n], beta2_param:obs2[n], beta3_param:obs3[n], theta_param:para})) 
          b2.add_points(state_to_bloch(state_2))
        b2.show()


        plot_rewards.append(episode_reward)
        rs.append(episode_reward)
        avg = np.mean(rs)
        avg_reward.append(avg)
        if i >= windows:
            if avg > best_avg_reward:
                best_avg_reward = avg

        print("\rEpisode {}/{} || Best average reward {}, Current Average {}, Current Iteration Reward {}".format(i,
                                                                                                                  ITERATIONS,
                                                                                                                  best_avg_reward,
                                                                                                                  avg,
                                                                                                                  episode_reward),
              )
    print(b1)
    np.save("VQC PPO HRyRyRz Rx measurement 32 1qubits  (lr=0.004) rewards", np.asarray(plot_rewards))

    plt.title("VQC PPO HRzRyRz Rx  ")
    plt.plot(plot_rewards, label='Reward')
    plt.plot(avg_reward, label='Average')
    plt.legend()
    plt.ylabel('Reward')
    plt.xlabel('Iteration')
    plt.show()